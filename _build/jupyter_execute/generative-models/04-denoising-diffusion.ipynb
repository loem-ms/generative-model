{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diffusion model is also a kind of latent variable model. It can also considered as a revolutionary version of the hierarchy VAE. However, with only the following changes in the hierarchy VAE, the diffusion model can be easily implemented but with surprisingly generative performance. \n",
    "\n",
    "1. **Latent and Observed variables have the same number of dimensions.**\n",
    "2. **The encoder is just a (Gaussian) noise-adding module, where the noise is generated from a fixed normal distribution.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 9](../figures/part3-denoising-diffusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of the diffusion model is to learn the distribution of observed data from the ***denoising process***. The diffusion model's two primary phases are ***diffusion*** and ***denoising***.\n",
    "\n",
    "- In the diffusion process, the observed data is corrupted by repeatedly adding noise generated from a fixed normal distribution until it becomes a completely random noise.\n",
    "- Then, in the denoising process, a neural network learns to predict the noise added to the data, and with the denoising process, the data is restored (generated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diffusion](../figures/part3-diffusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diffusion process, noise is gradually added to the data from the previous time. The noise addition should be set so that at the final time the data is destroyed as latent variables. In general, the latent variable at the final step is assumed to follow a fixed normal distribution such as $\\mathcal{N}(x_T;0,\\text{I})$. This is similar to the assumption in VAE where the latent variable is generated from a normal distribution. \n",
    "\n",
    "The modeling of the diffusion process at each time step $1\\leq t \\leq T$ is represented as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q(x_t|x_{t-1}) = \\mathcal{N}(x_t;\\sqrt{1-\\beta_t}x_{t-1}, \\beta_t\\text{I})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\beta_t$ is set to a small value such as 0.001 and $T$ is set sufficiently large, the distribution of $x_t$ can be made close to a normal distribution.\n",
    "\n",
    "In fact, $x_t$ can be obtained using the following variable transformation trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\epsilon\\sim\\mathcal{N}(\\epsilon;0,\\text{I})\\\\x_t = \\sqrt{1-\\beta_t}x_{t-1}+\\sqrt{\\beta_t}\\epsilon$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the denoising process, a neural network is used. In the diffusion model, all latent and observed variables have the same number of dimensions, so a common neural network can be used for denoising at each time point. Denoising can be modeled as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{x}_{t-1} = \\text{NeuralNetwork}(x_t,t;\\theta)\\\\p_\\theta(x_{t-1}|x_t) =\\mathcal{N}(x_{t-1};\\hat{x}_{t-1},\\text{I})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters estimation in a diffusion model is also performed by maximizing the log-likelihood function. In the diffusion model, the parameters are those of the neural network used in the denoising process. However, since it is difficult to compute the log-likelihood of a diffusion model, similar to the VAE, ELBO is used.\n",
    "\n",
    "First, let’s calculate the ELBO of the diffusion model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\\text{ELBO}(x_0;\\theta) &= \\int q(x_{1:T}|x_0)\\log\\frac{p_{\\theta}(x_{0:T})}{q(x_{1:T}|x_0)}dx_{1:T} \\\\\n",
    "&=  \\mathbb{E}_{q(x_{1:T}|x_0)}\\left[\\log\\frac{p_{\\theta}(x_{0:T})}{q(x_{1:T}|x_0)}\\right] \\\\&= \\mathbb{E}_{q(x_{1:T}|x_0)}\\left[\\log\\frac{p(x_{T})\\prod_{t=1}^{T}{p_{\\theta}(x_{t-1}|x_t)}}{\\prod_{t=1}^{T}{q(x_{t}|x_t-1)}}\\right] \\\\&= \\mathbb{E}_{q(x_{1:T}|x_0)}\\left[\\log{\\prod_{t=1}^{T}{p_{\\theta}(x_{t-1}|x_t)}} + \\log{\\frac{p(x_T)}{\\prod_{t=1}^{T}{q(x_{t}|x_t-1)}}}\\right]\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, since the last time step data is a complete noise which follow a normal distribution, $p(x_T) $ is not a function of $\\theta$. Therefore, maximizing ELBO is equivalent to maximizing the following function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}    J(\\theta) &= \\mathbb{E}_{q(x_{1:T}|x_0)}\\left[\\log{\\prod_{t=1}^{T}{p_{\\theta}(x_{t-1}|x_t)}} \\right] \\\\    &= \\mathbb{E}_{q(x_{1:T}|x_0)}\\left[{\\sum_{t=1}^{T}\\log{p_{\\theta}(x_{t-1}|x_t)}} \\right] \\\\    &= \\sum_{t=1}^{T}\\mathbb{E}_{q(x_{1:T}|x_0)}\\left[{\\log{p_{\\theta}(x_{t-1}|x_t)}} \\right] \\\\    &= \\sum_{t=1}^{T}\\mathbb{E}_{q(x_{t-1}, x_t|x_0)}\\left[{\\log{p_{\\theta}(x_{t-1}|x_t)}} \\right] \\\\    \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{E}_{q(x_{t-1}, x_t|x_0)}[\\cdot]$ can be approximated with Monte Carlo method and together with the modeling of denoising process, this function can be further approximated as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "J(\\theta) &\\approx \\sum_{t=1}^{T}\\log p_\\theta(x_{t-1}|x_t) \\\\ \\\\    \n",
    "\\hat{x}_{t-1} &= \\text{NeuralNet}(x_t,t;\\theta) \\\\    p_\\theta(x_{t-1}|x_t) &= \\mathcal{N}(x_{t-1};\\hat{x}_{t-1},\\mathbf{I})\\\\ \\\\    J(\\theta) &\\approx -\\frac{1}{2}\\sum_{t=1}^{T}||x_{t-1}-\\hat{x}_{t-1}||^2    \\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Estimate Latent Variable](../figures/part3-diffusion-est-latent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing this $J(\\theta)$ is equivalent to minimizing loss between $x_{t-1}$ and denoised $\\hat{x}_{t-1}$. This means we train the neural network to conduct the denoising process given time step $t$ and corresponding data $x_t $ as inputs. However, as you may have noticed, each time of $J(\\theta)$ calculation, we need to do $T$ times diffusion process sampling. This leads to high computational cost when we have large value of $T$ set. To make the process more efficient, another method of approximation method can be considered by using only one sampling of the diffusion process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this method is to train the neural network in denoising process to predict the noise $\\hat\\epsilon$added to the data, instead of predicting the data $\\hat{x}_{t-1}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![noise prediction](../figures/part3-diffusion-est-noise.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will mathematical show that training the neural network to predict the noise is equivalent to the previous approximation with $T$ times samplings in the previous section. \n",
    "\n",
    "Note that the distribution $p_\\theta(x_{t-1}|x_t)$  can be modeled as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "p_\\theta(x_{t-1}|x_t)  &= \\mathcal{N}(x_{t-1};\\mu_\\theta(x_t,t),\\sigma_{q}^2(t)\\text{I})\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\mu_\\theta(x_t,t) &= \\text{NeuralNetwork}(x_t, t)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start from the modeling of the diffusion process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "q(x_t|x_{t-1}) &= \\mathcal{N}(x_t;\\sqrt{{\\alpha}_t}x_{t-1},(1-\\alpha_t)\\text{I}) ~~~,\\text{where}~\\alpha_t = 1-\\beta_t \\\\\n",
    "q(x_t|x_0) &= \\mathcal{N}(x_t;\\sqrt{\\bar{\\alpha}_t}x_0,(1-\\bar\\alpha_t)\\text{I}) ~~~,\\text{where}~\\bar\\alpha_t = \\prod_{t=1}^{T}(1-\\beta_t) \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, $x_t$ can be sampled as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\epsilon &\\sim \\mathcal{N}(0, \\text{I}) \\\\\n",
    "x_t &= \\sqrt{\\bar\\alpha_t}x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can express the original observed data $x_0$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "x_0 &= \\frac{x_t-\\sqrt{1-\\bar\\alpha_t}\\epsilon}{\\sqrt{\\bar\\alpha_t}}  \n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $x_0$ and $x_t$, the distribution of $x_{t-1}$ can be obtained as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "q(x_{t-1}|x_t,x_0) &= \\frac{q(x_t|x_{t-1},x_0)q(x_{t-1}|x_0)}{q(x_t|x_0)}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Markov assumption, $q(x_t|x_{t-1},x_0) = q(x_t|x_{t-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "q(x_{t-1}|x_t,x_0) &= \\frac{q(x_t|x_{t-1})q(x_{t-1}|x_0)}{q(x_t|x_0)} \\\\\n",
    "&=\\frac{\\mathcal{N}(x_t;\\sqrt{{\\alpha}_t}x_{t-1},(1-\\alpha_t)\\text{I}) ~ \\mathcal{N}(x_{t-1};\\sqrt{\\bar{\\alpha}_{t-1}}x_0,(1-\\bar\\alpha_{t-1})\\text{I}) }{\\mathcal{N}(x_t;\\sqrt{\\bar{\\alpha}_t}x_0,(1-\\bar\\alpha_t)\\text{I}) }\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By expanding the right side of this equation, we obtain the following distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "q(x_{t-1}|x_t,x_0) &= \\mathcal{N}\\left(x_{t-1}; \\mu_q(x_t,x_0), \\sigma_q^2(t)\\text{I}\\right) \\\\\n",
    "\\mu_q(x_t,x_0) &= \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon\\right) \\\\\n",
    "\\sigma_q^2(t) &= \\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, let’s rewrite $\\mu_\\theta(x_t,t)$ to match the form of $\\mu_q(x_t,x_0)$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\mu_\\theta(x_t,t) &= \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(x_t, t)\\right)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", where now we set the output of the neural network to $\\epsilon_\\theta(x_t, t)$ instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s look back to the KL divergence between the two normal distributions $q(x_{t-1}|x_t,x_0)$ and $p_\\theta(x_{t-1}|x_t)= \\mathcal{N}(x_{t-1};\\hat{x}_{t-1},\\mathbf{I})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "D_{\\text{KL}}(q(x_{t-1}|x_t,x_0)\\|p_\\theta(x_{t-1}|x_t)) &= \\frac{1}{2\\sigma_q^2(t)}\\|\\mu_\\theta(x_t,t) - \\mu_q(x_t,x_0)\\|^2 \\\\\n",
    "&= \\frac{1}{2\\sigma_q^2(t)}\\|\\epsilon_\\theta(x_t,t) - \\epsilon\\|^2\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that by train the neural network to predict the noise, which is minimizing the above KL divergence, lead to maximizing the EBOL of the diffusion model. Thus, this optimization is equivalent to the parameter estimation with $T$ times sampling in the previous section.\n",
    "\n",
    "To sum up, the training algorithm of the diffusion model is as follows.\n",
    "\n",
    "Repeat the following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the following steps:\n",
    "\n",
    "1. Randomly sample a training data instance\n",
    "2. Sample a time step $t$ from a uniform distribution : $t \\sim U[1,T]$ \n",
    "3. Generate Gaussian noise for diffusion process: $\\epsilon \\sim \\mathcal{N}(0,\\text{I})$\n",
    "4. Diffusion process: $x_t = \\sqrt{\\bar\\alpha_t}x_0 + \\sqrt{1-\\bar\\alpha_t}\\epsilon$\n",
    "5. Calculate loss: $\\text{Loss}(x_0, \\theta) = \\|\\epsilon_\\theta(x_t,t)-\\epsilon\\|^2$\n",
    "6. Update parameters $\\theta$ with gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in previous sections, the denoising process is modeled as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1};\\mu_\\theta(x_t,t),\\sigma_q^2(t)\\text{I})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the following trick, we can sample every time step data with the following process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\epsilon &\\sim \\mathcal{N}(0, I) \\\\\n",
    "x_{t-1} &= \\mu_\\theta(x_t,t)+\\sigma_q(t)\\epsilon\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align*}\n",
    "\\mu_\\theta(x_t,t) &= \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(x_t, t)\\right) \\\\\n",
    "\\sigma_q(t)&=\\sqrt{\\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". Thus, the process of new data generation can be summarized as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generation Process**\n",
    "\n",
    "$x_T \\sim \\mathcal{N}(0,\\text{I})$\n",
    "\n",
    "for $t$ in $[T,...,1]$:\n",
    "\n",
    "$\\epsilon\\sim\\mathcal{N}(0,\\text{I})$\n",
    "\n",
    "if $t=1$ then $\\epsilon=0$\n",
    "\n",
    "$\\sigma_q(t)=\\sqrt{\\frac{(1-\\alpha_t)(1-\\bar\\alpha_{t-1})}{1-\\bar\\alpha_t}}$\n",
    "\n",
    "$x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar\\alpha_t}}\\epsilon_\\theta(x_t, t)\\right) +\\sigma_q(t)\\epsilon$\n",
    "\n",
    "return $x_0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diffusion generated](../figures/part3-diffusion-generated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}